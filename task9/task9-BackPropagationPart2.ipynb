{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation - Boston Housing Dataset \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "HousingData = 'HousingData.csv'\n",
    "dataset = pd.read_csv(HousingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features (X) and target variable (y)\n",
    "X = dataset.drop('CRIM', axis=1)\n",
    "y = dataset['CRIM'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "X.fillna(X.mean(), inplace=True)  # Replace missing values with the column mean\n",
    "\n",
    "# Convert the dataset to a numpy array\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize input variables\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the functions and algorithms\n",
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights': np.random.uniform(size=n_inputs + 1)} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights': np.random.uniform(size=n_hidden + 1)} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with a network\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return np.argmax(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = neuron['weights'][-1]\n",
    "            for i in range(len(neuron['weights']) - 1):\n",
    "                activation += neuron['weights'][i] * inputs[i]\n",
    "            neuron['output'] = sigmoid(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = []\n",
    "        if i != len(network) - 1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * sigmoid_derivative(neuron['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update network weights with error\n",
    "def update_weights(network, row, learning_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += learning_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += learning_rate * neuron['delta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1.0 - x)\n",
    "\n",
    "# Calculate accuracy\n",
    "def calculate_accuracy(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation Algorithm With Stochastic Gradient Descent\n",
    "def back_propagation(train, test, l_rate, n_epoch, n_hidden):\n",
    "    n_inputs = train.shape[1] - 1\n",
    "    n_outputs = len(set(train[:, -1])) if len(set(train[:, -1])) > len(set(test[:, -1])) else len(set(test[:, -1]))\n",
    "    network = initialize_network(n_inputs, n_hidden, n_outputs)\n",
    "    for epoch in range(n_epoch):\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = np.zeros(n_outputs)\n",
    "            if not np.isnan(row[-1]) and int(row[-1]) < n_outputs:\n",
    "                expected[int(row[-1])] = 1\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "    predictions = []\n",
    "    for row in test:\n",
    "        prediction = predict(network, row)\n",
    "        predictions.append(prediction)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target variable to integers\n",
    "class_values = list(set(y_train))\n",
    "lookup = {value: i for i, value in enumerate(class_values)}\n",
    "y_train = np.array([lookup[value] if not pd.isnull(value) else np.nan for value in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the features and target variable for training\n",
    "train_data = np.column_stack((X_train, y_train))\n",
    "\n",
    "# Convert the target variable to integers for testing\n",
    "y_test = np.array([lookup.get(value, np.nan) if not pd.isnull(value) else np.nan for value in y_test])\n",
    "\n",
    "# Combine the features and target variable for testing\n",
    "test_data = np.column_stack((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate algorithm\n",
    "def evaluate_algorithm(data, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(data, n_folds)\n",
    "    scores = []\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = np.concatenate(train_set)\n",
    "        test_set = fold\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = test_set[:, -1]\n",
    "        accuracy = calculate_accuracy(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "\n",
    "def cross_validation_split(data, n_folds):\n",
    "    fold_size = len(data) // n_folds\n",
    "    data_copy = list(data)\n",
    "    folds = []\n",
    "    for _ in range(n_folds):\n",
    "        fold = []\n",
    "        while len(fold) < fold_size:\n",
    "            index = np.random.choice(len(data_copy))\n",
    "            fold.append(data_copy.pop(index))\n",
    "        folds.append(np.array(fold))\n",
    "    return folds\n",
    "\n",
    "def calculate_accuracy(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "n_folds = 5\n",
    "l_rate = 0.3\n",
    "n_epoch = 500\n",
    "n_hidden = 5\n",
    "scores = evaluate_algorithm(train_data, back_propagation, n_folds, l_rate, n_epoch, n_hidden)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores) / float(len(scores))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
